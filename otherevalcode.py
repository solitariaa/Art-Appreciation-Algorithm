# -*- coding: utf-8 -*-
"""“Report3.ipynb”的副本

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aW1mUnHFqRA_LVxL4DJ8SWnGR_6f7rMJ

# DataSet
"""

from google.colab import drive
drive.mount('/content/drive')
!unzip "/content/drive/Shareddrives/COMP 646/SemArt.zip" -d "/content/"

from torch.utils.data import Dataset, DataLoader
from PIL import Image
import pandas as pd
import os

class SemArtDataset(Dataset):
    def __init__(self, csv_file, img_dir, transform=None):
        """
        csv_file: 数据集csv文件的路径
        img_dir: 图像文件的根目录
        transform: 图像的转换操作
        """
        self.data_frame = pd.read_csv(csv_file, sep="\t", encoding="latin-1")
        self.img_dir = img_dir
        self.transform = transform

    def __len__(self):
        return len(self.data_frame)

    def __getitem__(self, idx):
        # 加载图像
        img_name = os.path.join(self.img_dir, self.data_frame.iloc[idx, 0])
        image = Image.open(img_name).convert("RGB")
        if self.transform:
            image = self.transform(image)

        # 准备文本输入和输出
        author = self.data_frame.iloc[idx, 2]
        title = self.data_frame.iloc[idx, 3]
        technique = self.data_frame.iloc[idx, 4]
        date = self.data_frame.iloc[idx, 5]
        type_ = self.data_frame.iloc[idx, 6]
        school = self.data_frame.iloc[idx, 7]
        description = self.data_frame.iloc[idx, 1]

        #tokenizer
        #author = tokenizer(self.data_frame.iloc[idx, 2], truncation=True, padding='max_length', max_length=512, return_tensors="pt")
        #title = tokenizer(self.data_frame.iloc[idx, 3], truncation=True, padding='max_length', max_length=512, return_tensors="pt")
        #technique = tokenizer(self.data_frame.iloc[idx, 4], truncation=True, padding='max_length', max_length=512, return_tensors="pt")
        #date = tokenizer(self.data_frame.iloc[idx, 5], truncation=True, padding='max_length', max_length=512, return_tensors="pt")
        #type_ = tokenizer(self.data_frame.iloc[idx, 6], truncation=True, padding='max_length', max_length=512, return_tensors="pt")
        #school = tokenizer(self.data_frame.iloc[idx, 7], truncation=True, padding='max_length', max_length=512, return_tensors="pt")
        #description = tokenizer(self.data_frame.iloc[idx, 1], truncation=True, padding='max_length', max_length=512, return_tensors="pt")

        return image, img_name, author, title, technique, date, type_, school, description

from torchvision import transforms
from torch.utils.data import Dataset, DataLoader

# 定义图像预处理操作
transform = transforms.Compose([
    transforms.Resize((254, 254)),  # 调整图像大小
    transforms.ToTensor(),  # 将图像转换为PyTorch张量
    transforms.Normalize(mean=[0.485, 0.456, 0.406],  # 归一化图像
                         std=[0.229, 0.224, 0.225])
])
train_data = SemArtDataset(csv_file='/content/SemArt/semart_train.csv',
                           img_dir='/content/SemArt/Images/',
                           transform=transform)
test_data = SemArtDataset(csv_file='/content/SemArt/semart_test.csv',
                           img_dir='/content/SemArt/Images/',
                           transform=transform)
val_data = SemArtDataset(csv_file='/content/SemArt/semart_val.csv',
                           img_dir='/content/SemArt/Images/',
                           transform=transform)
batch_size = 8

train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)
test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)

import matplotlib.pyplot as plt
import torch
def show_images(dataset, num_images=4):
    """
    从数据集中随机选择并展示几个图像及其相关信息。
    :param dataset: 数据集对象
    :param num_images: 要展示的图像数量
    """
    fig, axs = plt.subplots(1, num_images, figsize=(15, 15))
    for i in range(num_images):
        # 随机选择一个索引
        idx = 1
        # 获取图像和相关信息
        image, img_name, author, title, technique, date, type_, school, description = dataset[idx]

        # 转换图像张量以便于显示
        image = image.permute(1, 2, 0)  # 将通道从[C, H, W]变换到[H, W, C]
        image = (image - image.min()) / (image.max() - image.min())  # 归一化到[0, 1]

        # 显示图像
        axs[i].imshow(image)
        axs[i].axis('off')

        # 打印相关信息
        print(f"Image {i+1}:")
        print(f"Author: {author}")
        print(f"Title: {title}")
        print(f"Technique: {technique}")
        print(f"Date: {date}")
        print(f"Type: {type_}")
        print(f"School: {school}")
        print(f"Description: {description}\n")

    plt.show()


# 展示train_data中的几个图像及其相关信息
show_images(train_data, num_images=4)

"""# Model-BLIP2"""

import sys
if 'google.colab' in sys.modules:
    print('Running in Colab.')
    !pip3 install salesforce-lavis

import torch
from PIL import Image
import requests
from lavis.models import load_model_and_preprocess
device = torch.device("cuda") if torch.cuda.is_available() else "cpu"
model, vis_processors, _ = load_model_and_preprocess(
    name="blip2_opt", model_type="caption_coco_opt2.7b", is_eval=True, device=device
    #name="blip2_t5", model_type="pretrain_flant5xl", is_eval=True, device=device
)
vis_processors.keys()

"""run blip"""

prompt = (
    #f"This artwork titled '{title}', created by {author}, is a remarkable example of the {school} school of art. "
    #f"Crafted with {technique} around {date}, this piece is a {type} that stands out in history. "
    f"appreciate the pating '{title} by {author}:"
)

idx = 100
    # Get the image and related information
image, img_name, author, title, technique, date, type_, school, description = train_data[idx]


image = image.unsqueeze(0).to(device, torch.float32)

prompt = (
    #f"Title: '{title}' and artist: '{author}'. "
    f"Appreciate the {type_} painting named '{title}' crafted with {technique}:"
)

#prompt = "the author of this painting is HOLBEIN, appreciate the painting:"

print(prompt)

"""# train"""

!pip install wandb --quiet
import torch
from torch import nn, optim
from torch.utils.data import DataLoader
from tqdm.auto import tqdm
import json
import time
import wandb
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()

# 初始化 Weights & Biases 项目
wandb.init(project="blit2_train", entity="xxw38")
project_name = 'blit2_train'

# 检查 GPU 使用情况
device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
print(f'Use {torch.cuda.device_count()} GPUs' if torch.cuda.device_count() > 0 else 'Use CPU')

criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)
num_epochs = 10

torch.cuda.empty_cache()
model.to(device)
#model.load_state_dict(torch.load('/taiga/moonshot/blip2_train_fp16_t5/result/pth/epoch1_t5.pth'))
model.max_txt_len = 480
model = model.to(torch.float32)

# 训练循环
train_loss_list = []
val_loss_list = []

for epoch in range(num_epochs):
  for batch in train_loader:
    start_time = time.time()
    model.train()
    sum_train_loss = 0.0
    sum_val_loss = 0.0
    train_count = 0
    val_count = 0
    best_val_loss = float('inf')
    best_model_state = None

    for image, img_name, author, title, technique, date, type_, school, description in tqdm(train_loader, desc=f'[Train Epoch: {epoch}]'):
    #for image, describe, author, title, technique in tqdm(train_loader, desc=f'[Train Epoch: {epoch}]'):
    #for images, input_prompts in tqdm(zip(train_images, train_texts), desc=f'[Train Epoch: {epoch}]', total=len(train_images)):
        print(image.shape)
        train_count += 1
        #images = images.unsqueeze(0)
        #images = images.float()
        samples = {'image':image.to(device, torch.float32), 'text_input':description, 'text_output':description}
        outputs = model(samples)

        loss = outputs['loss']
        sum_train_loss += loss.item()
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        wandb.log({"iter_loss": loss.item()})

    # 计算平均损失
    ave_loss = sum_train_loss / len(train_count)
    train_loss_list.append(ave_loss)

    # 记录和输出训练信息
    wandb.log({"epoch": epoch, "train_loss": ave_loss})
    print(f"Epoch: {epoch}, Train Loss: {ave_loss}, Time: {time.time() - start_time}s")

    # 模型保存
    if epoch % 10 == 0:
        torch.save(model.state_dict(), f"result/pth/{project_name}_epoch{epoch}.pth")

    predictions = []
    references = []

    with torch.no_grad():  # 关闭梯度计算
        for image, describe, author, title, technique in tqdm(train_loader, desc=f'[Train Epoch: {epoch}]'):
        #for val_images, val_input_prompts in tqdm(zip(val_images, val_texts), desc=f'[Validation Epoch: {epoch}]', total=len(val_images)):
            val_count += 1
            val_images = image.unsqueeze(0).to(device, torch.float32)
            val_samples = {'image': val_images, 'text_input': describe, 'text_output': describe}
            val_outputs = model(val_samples)
            val_loss = val_outputs['loss']
            sum_val_loss += val_loss.item()

            predictions.extend(val_outputs['predictions'])
            references.extend(describe)

    ave_val_loss = sum_val_loss / val_count
    val_loss_list.append(ave_val_loss)
    wandb.log({"epoch": epoch, "val_loss": ave_loss})
    print(f"Epoch: {epoch}, Val Loss: {ave_loss}, Time: {time.time() - start_time}s")

    rouge_scores = calculate_rouge_scores(predictions, references)
    print(f"ROUGE scores for epoch {epoch}: {rouge_scores}")

    if ave_val_loss < best_val_loss:
        best_val_loss = ave_val_loss
        best_model_state = model.state_dict()
        # 保存最佳模型
        torch.save(model.state_dict(), f"result/pth/{project_name}_best_model.pth")

# 保存训练损失到JSON
with open(f'result/{project_name}_train_loss_list.json', 'w') as file:
    json.dump(train_loss_list, file)
if best_model_state:
    torch.save(best_model_state, f"result/pth/{project_name}_best_model_final.pth")
wandb.finish()

"""# llava"""

!pip install --upgrade -q accelerate bitsandbytes
!pip install git+https://github.com/huggingface/transformers.git

# Commented out IPython magic to ensure Python compatibility.
import locale
def getpreferredencoding(do_setlocale = True):
  return "UTF-8"
locale.getpreferredencoding = getpreferredencoding

!wget https://files.pythonhosted.org/packages/source/a/accelerate/accelerate-0.2.0.tar.gz
!tar xzf accelerate-0.2.0.tar.gz
# %cd accelerate-0.2.0
!{sys.executable} setup.py install

from transformers import AutoProcessor, LlavaForConditionalGeneration
from transformers import BitsAndBytesConfig
import torch
import accelerate
print("Accelerate version:", accelerate.__version__)

quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16
)


model_id = "llava-hf/llava-1.5-7b-hf"

processor = AutoProcessor.from_pretrained(model_id)
model = LlavaForConditionalGeneration.from_pretrained(model_id, quantization_config=quantization_config, device_map="auto")

def generate_output(image, author, title, technique, date, image_type, school):
    # Resize the image to reduce its size
    #resized_image = image.resize((image.width // 2, image.height // 2))  # Resize to half the original size

    # Display the resized image
    #display(resized_image)

    # Additional text
    additional_text = f"This painting created by {author} around {date}, named '{title}' and crafted with {technique}, is a remarkable example of the {school} school of art."

    # Prompt for the LLAMA model
    prompt = f"USER: <image>\n{additional_text}\n Can you shortly describe this {image_type} painting.\nASSISTANT:"

    # Prepare inputs for the LLAMA model
    inputs = processor(prompt, images=image, padding=True, return_tensors="pt").to("cuda")

    # Generate text based on the image
    output = model.generate(**inputs, max_new_tokens=1000)
    generated_text = processor.batch_decode(output, skip_special_tokens=True)

    return generated_text
    # Display the generated text

import time
start_time = time.time()
device = torch.device("cuda") if torch.cuda.is_available() else "cpu"

idx = 10
image, img_name, author, title, technique, date, type_, school, description = train_data[idx]
image = image.unsqueeze(0).to(device, torch.float32)

image_pil = Image.fromarray(torch.mul(image.squeeze(0), 255).byte().cpu().numpy().transpose((1, 2, 0)), 'RGB')

generate = generate_output(image_pil, author, title, technique, date, type_, school)

generated_texts = []
for text in generate:
    protext = text.split("ASSISTANT:")[-1]
    generated_texts.append(protext.strip())


formatted_output = "\n".join(generated_texts)
print(formatted_output)
end_time = time.time()
elapsed_time = end_time - start_time

print("Execution Time:", elapsed_time, "seconds")
#print(output)

"""# rouge"""

import locale
def getpreferredencoding(do_setlocale = True):
  return "UTF-8"

locale.getpreferredencoding = getpreferredencoding
!pip install rouge
from rouge import Rouge
from collections import defaultdict

def calculate_rouge_1_scores(predictions, references):
    rouge = Rouge()
    # 计算所有ROUGE得分
    scores = rouge.get_scores(predictions, references, avg=True)
    # 只提取ROUGE-1得分
    rouge_1_scores = scores['rouge-1']
    return rouge_1_scores

def generate_non_empty_caption(model, image, prompt):
    while True:
        msgs = [{'role': 'user', 'content': prompt}]

        res, context, _ = modelCPM.chat(
        image=image_pil,
        msgs=msgs,
        context=None,
        tokenizer=tokenizer,
        sampling=True,
        temperature=0.7
        )


        if res and any(item.strip() for item in res):
            # Join words in the caption if it's in list form and return
            joined_caption = ''.join(res).split()
            final_caption = ' '.join(joined_caption)

            return [final_caption]
        #print("null")

from tqdm import tqdm
import time

all_f = []
all_p = []
all_r = []
device = torch.device("cuda") if torch.cuda.is_available() else "cpu"
start_time = time.time()

image, img_name, author, title, technique, date, type_, school, description = test_data[100]

image = image.unsqueeze(0).to(device, torch.float32)
image_pil = Image.fromarray(torch.mul(image.squeeze(0), 255).byte().cpu().numpy().transpose((1, 2, 0)), 'RGB')

#prompt = (f"This painting created by {author} around {date}, named '{title}' and crafted with {technique}, is a remarkable example of the {school} school of art, can you shortly describe this {type_} painting")

prompt = (f"can you provide basic information like author, title, school and appreciate this painting")
#print(title)
caption = generated_texts
#print(caption)

r = (f'{author}, {title}, {school}, {type_}')
#caption_reference = [r]
caption_reference = [description]
#print(caption_reference)
#caption_reference = [" ".join(item) for item in caption_reference]



score = calculate_rouge_1_scores(caption, caption_reference)

all_f.append(score['f'])
all_p.append(score['p'])
all_r.append(score['r'])

avg_f = sum(all_f) / len(all_f)
avg_p = sum(all_p) / len(all_p)
avg_r = sum(all_r) / len(all_r)

end_time = time.time()
elapsed_time = end_time - start_time

print("Execution Time:", elapsed_time, "seconds")

print("Average ROUGE-1 F1 Score:", avg_f)
print("Average ROUGE-1 P1 Score:", avg_p)
print("Average ROUGE-1 R1 Score:", avg_r)

from tqdm import tqdm
device = torch.device("cuda") if torch.cuda.is_available() else "cpu"

all_f = []
all_p = []
all_r = []

for i in range(1):
  image, img_name, author, title, technique, date, type_, school, description = test_data[10]

  image = image.unsqueeze(0).to(device, torch.float32)
  image_pil = Image.fromarray(torch.mul(image.squeeze(0), 255).byte().cpu().numpy().transpose((1, 2, 0)), 'RGB')

  caption = generated_texts
  #generate_output(image_pil, author, title, technique, date, type_, school)

  if len(caption) <= 0:
    print("skip")
    continue

  caption_reference = [description]
  #caption_reference = [" ".join(item) for item in caption_reference]

  print(caption)
  print(caption_reference)
  score1 = calculate_rouge_1_scores(caption, caption_reference)

  print(score1)
  all_f.append(score1['f'])
  all_p.append(score1['p'])
  all_r.append(score1['r'])
  print(len(all_f))

  #cur_f = sum(all_f) / len(all_f)
  #cur_p = sum(all_p) / len(all_p)
  #cur_r = sum(all_r) / len(all_r)

avg_f = sum(all_f) / len(all_f)
avg_p = sum(all_p) / len(all_p)
avg_r = sum(all_r) / len(all_r)

print("Average ROUGE-1 F1 Score:", avg_f)
print("Average ROUGE-1 P1 Score:", avg_p)
print("Average ROUGE-1 R1 Score:", avg_r)

"""# METEOR score"""

!pip install nltk
import nltk
from nltk.translate.meteor_score import meteor_score

nltk.download('wordnet')
nltk.download('omw-1.4')
nltk.download('wordnet')
nltk.download('punkt')

def generate_caption(model, image, prompt):
    while True:
        msgs = [{'role': 'user', 'content': prompt}]

        res, context, _ = modelCPM.chat(
        image=image_pil,
        msgs=msgs,
        context=None,
        tokenizer=tokenizer,
        sampling=True,
        temperature=0.7
        )


        if res and any(item.strip() for item in res):
            # Join words in the caption if it's in list form and return
            joined_caption = ''.join(res).split()
            final_caption = ' '.join(joined_caption)

            return final_caption

image, img_name, author, title, technique, date, type_, school, description = test_data[10]

image = image.unsqueeze(0).to(device, torch.float32)
image_pil = Image.fromarray(torch.mul(image.squeeze(0), 255).byte().cpu().numpy().transpose((1, 2, 0)), 'RGB')

prompt = ( f"This painting created by {author} around {date}, named '{title}' and crafted with {technique}, is a remarkable example of the {school} school of art, can you shortly describe and analyze the artistic style of this {type_} painting")

#prompt = (f"can you shortly appreciate this painting")
#print(title)
caption = generate_output(image_pil, author, title, technique, date, type_, school)

caption = "\n".join(caption)

caption_reference = description


print(caption)
print(caption_reference)

caption_tokens = nltk.word_tokenize(caption.lower())
reference_tokens = nltk.word_tokenize(caption.lower())

score = meteor_score([reference_tokens], caption_tokens)
print("METEOR Score:", score)



"""# blip-运行"""

idx = 0
    # Get the image and related information
image, img_name, author, title, technique, date, type_, school, description = train_data[idx]


image = image.unsqueeze(0).to(device, torch.float32)
print(type(image))

context = [
    ("title of the painting?", title),
    ("author of the painting?", author),
]
question = "why the author is him:"
template = "Question: {} Answer: {}."

prompt = " ".join([template.format(context[i][0], context[i][1]) for i in range(len(context))]) + " Question: " + question + " Answer:"

print(prompt)

model.generate(
    {
    "image": image,
    "prompt": prompt
    },
    use_nucleus_sampling=True,
    num_captions=1,
)

caption = model.generate(
    {
    "image": image,
    "prompt": prompt
    },
    use_nucleus_sampling=True,
    num_captions=1,
)
print("Final Captions:", caption)
caption = [" ".join(item) for item in caption]

caption_reference = [description]
print("Captions_r:", caption_reference)
caption_reference = [" ".join(item) for item in caption_reference]

score = calculate_rouge_1_scores(caption, caption_reference)
print(f"ROUGE Scores: {score}")

model.eval()

def generate_non_empty_caption(model, image, prompt):
    while True:
        caption = model.generate(
            {
                "image": image,
                "prompt": prompt
            },
            use_nucleus_sampling=True,
            num_captions=1,
        )
        #print(caption)
        # Check if the generated caption is non-empty
        if caption and any(item.strip() for item in caption):
            # Join words in the caption if it's in list form and return
            return [" ".join(item.strip().split()) for item in caption]
        #print("null")

all_f = []
all_p = []
all_r = []
image, img_name, author, title, technique, date, type_, school, description = train_data[100]
image = image.unsqueeze(0).to(device, torch.float32)

prompt = (
    f"This painting created by '{author}' around {date}, named '{title}' and crafted with {technique},"
    f"is a remarkable example of the {school} school of art, can you appreciate this {type_} painting?"
)



caption = generate_non_empty_caption(model, image, prompt)

caption_reference = [description]
print(caption_reference)

caption_reference = [" ".join(item) for item in caption_reference]

score = calculate_rouge_1_scores(caption, caption_reference)

all_f.append(score['f'])
all_p.append(score['p'])
all_r.append(score['r'])

print("Average ROUGE-1 F1 Score:", all_f)
print("Average ROUGE-1 P1 Score:", all_p)
print("Average ROUGE-1 R1 Score:", all_r)

all_f = []
all_p = []
all_r = []
image, img_name, author, title, technique, date, type_, school, description = train_data[1]
image = image.unsqueeze(0).to(device, torch.float32)


prompt = (
    f"generate 100 words:"
)


caption = generate_non_empty_caption(model, image, prompt)
print(caption)

caption_reference = [description]
print(caption_reference)

caption_reference = [" ".join(item) for item in caption_reference]

score = calculate_rouge_1_scores(caption, caption_reference)

all_f.append(score['f'])
all_p.append(score['p'])
all_r.append(score['r'])

print("Average ROUGE-1 F1 Score:", all_f)
print("Average ROUGE-1 P1 Score:", all_p)
print("Average ROUGE-1 R1 Score:", all_r)

!pip install tqdm
from tqdm import tqdm

all_f = []
all_p = []
all_r = []

for i in tqdm(range(len(test_loader)), desc="Processing Images"):
  image, img_name, author, title, technique, date, type_, school, description = train_data[i]
  image = image.unsqueeze(0).to(device, torch.float32)

  prompt = (
    f"Appreciate the {type_} painting named '{title}', created by '{author}' and crafted with {technique}:"
  )

  caption = generate_non_empty_caption(model, image, prompt)

  if len(caption) <= 0:
    print("skip")
    continue
  caption_reference = [description]

  caption_reference = [" ".join(item) for item in caption_reference]

  score = calculate_rouge_1_scores(caption, caption_reference)

  all_f.append(score['f'])
  all_p.append(score['p'])
  all_r.append(score['r'])

  cur_f = sum(all_f) / len(all_f)
  cur_p = sum(all_p) / len(all_p)
  cur_r = sum(all_r) / len(all_r)

  if i % 40 == 0 and i != 0:
    print("curr F1 Score:", cur_f)
    print("curr P1 Score:", cur_p)
    print("curr R1 Score:", cur_r)

avg_f = sum(all_f) / len(all_f)
avg_p = sum(all_p) / len(all_p)
avg_r = sum(all_r) / len(all_r)

print("Average ROUGE-1 F1 Score:", avg_f)
print("Average ROUGE-1 P1 Score:", avg_p)
print("Average ROUGE-1 R1 Score:", avg_r)

import torch
print(torch.cuda.memory_allocated())  # 已分配的内存#
print(torch.cuda.memory_reserved())

"""# MiniCPM"""

!pip install timm
import torch
from PIL import Image
from transformers import AutoModel, AutoTokenizer

modelCPM = AutoModel.from_pretrained('openbmb/MiniCPM-V', trust_remote_code=True)
tokenizer = AutoTokenizer.from_pretrained('openbmb/MiniCPM-V', trust_remote_code=True)
modelCPM.eval().cuda()

from PIL import Image
import numpy as np

device = torch.device("cuda") if torch.cuda.is_available() else "cpu"
idx = 1
image, img_name, author, title, technique, date, type_, school, description = train_data[idx]
image = image.unsqueeze(0).to(device, torch.float32)

image_pil = Image.fromarray(torch.mul(image.squeeze(0), 255).byte().cpu().numpy().transpose((1, 2, 0)), 'RGB')
question = (f"This painting created around {date}, is a remarkable example of the {school} school of art, describe and appreciate this {type_} painting")
print(title)
#question = (f"This painting created by {author} around {date}, named '{title}' and crafted with {technique}, is a remarkable example of the {school} school of art, can you describe and appreciate this {type_} painting?")
msgs = [{'role': 'user', 'content': question}]
print(msgs)

res, context, _ = modelCPM.chat(
    image=image_pil,
    msgs=msgs,
    context=None,
    tokenizer=tokenizer,
    sampling=True,
    temperature=0.7
)
print(res)

"""# finetune"""

!pip install 'ms-swift[llm]' -U

!pip uninstall -y datasets
!pip install datasets

!swift sft --model_type minicpm-v-3b-chat --dataset coco-mini-en-2

"""# rouge"""

def generate_non_empty_caption(model, image, prompt):
    while True:
        msgs = [{'role': 'user', 'content': prompt}]

        res, context, _ = modelCPM.chat(
        image=image_pil,
        msgs=msgs,
        context=None,
        tokenizer=tokenizer,
        sampling=True,
        temperature=0.7
        )


        if res and any(item.strip() for item in res):
            # Join words in the caption if it's in list form and return
            joined_caption = ''.join(res).split()
            final_caption = ' '.join(joined_caption)

            return final_caption
        #print("null")

from tqdm import tqdm
import time

all_f = []
all_p = []
all_r = []

start_time = time.time()

image, img_name, author, title, technique, date, type_, school, description = test_data[10]

image = image.unsqueeze(0).to(device, torch.float32)
image_pil = Image.fromarray(torch.mul(image.squeeze(0), 255).byte().cpu().numpy().transpose((1, 2, 0)), 'RGB')

#prompt = (f"This painting created by {author} around {date}, named '{title}' and crafted with {technique}, is a remarkable example of the {school} school of art, can you shortly describe this {type_} painting")

prompt = (f"can you provide basic information like author, title, school and appreciate this painting")
#print(title)
caption = generate_non_empty_caption(modelCPM, image, prompt)

r = (f'{author}, {title}, {school}, {type_}')
caption_reference = [r]
#caption_reference = [description]
print(caption_reference)
#caption_reference = [" ".join(item) for item in caption_reference]

print(caption)

score = calculate_rouge_1_scores(caption, caption_reference)

all_f.append(score['f'])
all_p.append(score['p'])
all_r.append(score['r'])

avg_f = sum(all_f) / len(all_f)
avg_p = sum(all_p) / len(all_p)
avg_r = sum(all_r) / len(all_r)

end_time = time.time()
elapsed_time = end_time - start_time

print("Execution Time:", elapsed_time, "seconds")

print("Average ROUGE-1 F1 Score:", avg_f)
print("Average ROUGE-1 P1 Score:", avg_p)
print("Average ROUGE-1 R1 Score:", avg_r)

from tqdm import tqdm

all_f = []
all_p = []
all_r = []

for i in tqdm(range(len(test_loader)), desc="Processing Images"):
  image, img_name, author, title, technique, date, type_, school, description = test_data[i]

  image = image.unsqueeze(0).to(device, torch.float32)
  image_pil = Image.fromarray(torch.mul(image.squeeze(0), 255).byte().cpu().numpy().transpose((1, 2, 0)), 'RGB')

  prompt = (
    f"This painting created by {author} around {date}, named '{title}' and crafted with {technique}, is a remarkable example of the {school} school of art, can you shortly appreciate this {type_} painting"
  )

  caption = generate_non_empty_caption(modelCPM, image, prompt)

  if len(caption) <= 0:
    print("skip")
    continue

  caption_reference = [description]
  caption_reference = [" ".join(item) for item in caption_reference]

  #print(caption)
  #print(caption_reference)
  score = calculate_rouge_1_scores(caption, caption_reference)

  all_f.append(score['f'])
  all_p.append(score['p'])
  all_r.append(score['r'])

  cur_f = sum(all_f) / len(all_f)
  cur_p = sum(all_p) / len(all_p)
  cur_r = sum(all_r) / len(all_r)

  if i % 30 == 0 and i != 0:
    print("curr F1 Score:", cur_f)
    print("curr P1 Score:", cur_p)
    print("curr R1 Score:", cur_r)

avg_f = sum(all_f) / len(all_f)
avg_p = sum(all_p) / len(all_p)
avg_r = sum(all_r) / len(all_r)

print("Average ROUGE-1 F1 Score:", avg_f)
print("Average ROUGE-1 P1 Score:", avg_p)
print("Average ROUGE-1 R1 Score:", avg_r)

"""# METEOR score"""

!pip install nltk
import nltk
from nltk.translate.meteor_score import meteor_score

nltk.download('wordnet')
nltk.download('omw-1.4')
nltk.download('wordnet')
nltk.download('punkt')

def generate_caption(model, image, prompt):
    while True:
        msgs = [{'role': 'user', 'content': prompt}]

        res, context, _ = modelCPM.chat(
        image=image_pil,
        msgs=msgs,
        context=None,
        tokenizer=tokenizer,
        sampling=True,
        temperature=0.7
        )


        if res and any(item.strip() for item in res):
            # Join words in the caption if it's in list form and return
            joined_caption = ''.join(res).split()
            final_caption = ' '.join(joined_caption)

            return final_caption

device = torch.device("cuda") if torch.cuda.is_available() else "cpu"

image, img_name, author, title, technique, date, type_, school, description = test_data[100]

image = image.unsqueeze(0).to(device, torch.float32)
image_pil = Image.fromarray(torch.mul(image.squeeze(0), 255).byte().cpu().numpy().transpose((1, 2, 0)), 'RGB')

#prompt = ( f"This painting created by {author} around {date}, named '{title}' and crafted with {technique}, is a remarkable example of the {school} school of art, can you shortly describe and analyze the artistic style of this {type_} painting")

prompt = (f"can you shortly appreciate this painting")
#print(title)
caption = generate_caption(modelCPM, image, prompt)


caption_reference = description


print(caption)
print(caption_reference)

caption_tokens = nltk.word_tokenize(caption.lower())
reference_tokens = nltk.word_tokenize(caption_reference.lower())

score = meteor_score([reference_tokens], caption_tokens)
print("METEOR Score:", score)

"""# self"""

import nltk
from collections import Counter
nltk.download('punkt')

def calculate_intersection(caption, caption_reference):
    # Tokenize the texts
    tokens_c = set(nltk.word_tokenize(caption.lower()))
    tokens_r = set(nltk.word_tokenize(caption_reference.lower()))

    # Calculate intersection of tokens
    intersection = tokens_c.intersection(tokens_r)
    score = len(intersection)/len(tokens_r)
    return intersection, score

device = torch.device("cuda") if torch.cuda.is_available() else "cpu"

image, img_name, author, title, technique, date, type_, school, description = test_data[10]

image = image.unsqueeze(0).to(device, torch.float32)
image_pil = Image.fromarray(torch.mul(image.squeeze(0), 255).byte().cpu().numpy().transpose((1, 2, 0)), 'RGB')

#prompt = (f"can you shortly appreciate this painting")
prompt = ( f"This painting created by {author} around {date}, named '{title}' and crafted with {technique}, is a remarkable example of the {school} school of art, can you shortly describe and analyze the artistic style of this {type_} painting")

# Assuming generated_texts is a list of generated descriptions (captions)
#caption =  generate_non_empty_caption(modelCPM, image, prompt) # Access the first item if it's a list containing one description
caption = formatted_output
r = (f'{author}, {date}, {title}, {technique}, {school}, {type_}')
caption_reference = description

print(caption)
print(caption_reference)

score, inter = calculate_intersection(caption, caption_reference)
print(score, inter)

"""# BLEU Score"""

import nltk
from nltk.translate.bleu_score import sentence_bleu
from nltk.translate.bleu_score import SmoothingFunction

image, img_name, author, title, technique, date, type_, school, description = test_data[10]

image = image.unsqueeze(0).to(device, torch.float32)
image_pil = Image.fromarray(torch.mul(image.squeeze(0), 255).byte().cpu().numpy().transpose((1, 2, 0)), 'RGB')

prompt = (
    f"This painting created by {author} around {date}, named '{title}' and crafted with {technique}, is a remarkable example of the {school} school of art, can you shortly describe and analyze the artistic style of this {type_} painting")

#prompt = (f"can you shortly appreciate this painting")
#print(title)
caption = generate_non_empty_caption(modelCPM, image, prompt)

r = (f'{author}, {date}, {title}, {technique}, {school}, {type_}')
caption_reference = [r]
print(caption)
print(caption_reference)


# Calculate BLEU Score
score = sentence_bleu(caption_reference, caption, smoothing_function=SmoothingFunction().method1)
print(f"BLEU Score: {score:.9f}")

"""# DRQA"""

!git clone https://github.com/facebookresearch/DrQA.git

# Commented out IPython magic to ensure Python compatibility.
# Change the directory to the DrQA folder
# %cd DrQA

# Install the required Python dependencies listed in requirements.txt
!pip install -r requirements.txt

# Run the setup script with 'develop' flag
!python setup.py develop

